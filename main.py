from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import urllib.parse
from bs4 import BeautifulSoup

@register("bot_vod", "appale", "ä»APIè·å–è§†é¢‘åœ°å€ï¼ˆä½¿ç”¨ /vod æˆ– /vodd + ç”µå½±åï¼‰", "1.1")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))

    async def _common_handler(self, event, api_urls, keyword):
        """åˆå¹¶å¤šAPIç»“æœçš„æ ¸å¿ƒé€»è¾‘"""
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}  # æŒ‰æ ‡å‡†åŒ–æ ‡é¢˜èšåˆç»“æœ
        ordered_titles = []   # ç»´æŠ¤æ ‡å‡†åŒ–æ ‡é¢˜çš„åŸå§‹é¡ºåº
        
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            encoded_keyword = urllib.parse.quote(keyword)
            query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue

                        html_content = await response.text()
                        parsed_items = self._parse_html(html_content)
                        
                        if parsed_items:
                            successful_apis += 1
                            # åˆå¹¶ç»“æœå¹¶ä¿æŒé¡ºåº
                            for normalized_title, original_title, url in parsed_items:
                                if normalized_title not in grouped_results:
                                    grouped_results[normalized_title] = {
                                        'original_title': original_title,
                                        'urls': []
                                    }
                                    ordered_titles.append(normalized_title)
                                grouped_results[normalized_title]['urls'].append(url)

            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue

        # æ„å»ºæœ€ç»ˆè¾“å‡º
        result_lines = []
        total_videos = sum(len(data['urls']) for data in grouped_results.values())
        
        for idx, normalized_title in enumerate(ordered_titles, 1):
            data = grouped_results[normalized_title]
            original_title = data['original_title']
            urls = data['urls']
            result_lines.append(f"{idx}. ã€{original_title}ã€‘")
            result_lines.extend([f"   ğŸ¬ {url}" for url in urls])

        if result_lines:
            header = [
                f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
                f"ğŸ“Š æ‰¾åˆ° {total_videos} æ¡èµ„æº",
                "â”" * 30
            ]
            footer = [
                "â”" * 30,
                "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
                "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
                "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
                "â”" * 30
            ]
            full_msg = "\n".join(header + result_lines + footer)
            yield event.plain_result(full_msg)
        else:
            yield event.plain_result(f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº")

    def _parse_html(self, html_content):
        """è§£æHTMLå¹¶è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆæ ‡å‡†åŒ–æ ‡é¢˜ï¼‰"""
        soup = BeautifulSoup(html_content, 'html.parser')
        video_items = soup.select('rss list video')[:self.records]
        
        parsed_data = []
        for item in video_items:
            original_title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
            normalized_title = original_title.lower().strip()  # æ ‡å‡†åŒ–å¤„ç†
            # æå–æ‰€æœ‰æ’­æ”¾é“¾æ¥
            for dd in item.select('dl > dd'):
                for url in dd.text.split('#'):
                    if url := url.strip():
                        parsed_data.append((normalized_title, original_title, url))
        return parsed_data

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šå½±è§†æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_vod, text):
            yield msg

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """æˆäººå†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("âš ï¸ æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_18, text):
            yield msg
