from astrbot.api.all import *
import aiohttp
import urllib.parse
import time
from bs4 import BeautifulSoup

@register("bot_vod", "appale", "å½±è§†èµ„æºæœç´¢æ’ä»¶", 2.0.1")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))
        self.pagination_cache = {}

    def _get_session_id(self, event):
        """è·¨å¹³å°ç”¨æˆ·æ ‡è¯†è·å–"""
        try:
            # å¾®ä¿¡å¼€æ”¾å¹³å°ä¸“ç”¨å­—æ®µ
            return event.origin_user_id
        except AttributeError:
            try:
                # æ ‡å‡†å­—æ®µå›é€€
                return event.get_sender_id()
            except AttributeError:
                # ç»ˆæä¿åº•æ–¹æ¡ˆ
                return f"{hash(str(event))}-{time.time()}"

    def _split_into_pages(self, header, result_lines, footer):
        """æ™ºèƒ½åˆ†é¡µé€»è¾‘"""
        pages = []
        current_page = []
        current_length = 0
        
        # è®¡ç®—åŸºç¡€é•¿åº¦
        header_footer = '\n'.join(header + footer)
        base_length = len(header_footer) + 2  # æ¢è¡Œç¬¦
        max_content_length = 1000 - base_length

        for line in result_lines:
            line_length = len(line) + 1  # åŒ…å«æ¢è¡Œç¬¦
            
            # å¼ºåˆ¶åˆ†é¡µæ¡ä»¶
            if current_length + line_length > max_content_length:
                # å¯»æ‰¾æœ€åä¸€ä¸ªm3u8é“¾æ¥
                split_index = None
                for i in reversed(range(len(current_page))):
                    if 'm3u8' in current_page[i].lower():
                        split_index = i + 1
                        break
                
                # å¦‚æœæ‰¾ä¸åˆ°åˆ™å¼ºåˆ¶åˆ†å‰²
                if split_index is None:
                    split_index = len(current_page)
                
                # ç”Ÿæˆåˆ†é¡µ
                pages.append('\n'.join(header + current_page[:split_index] + footer))
                current_page = current_page[split_index:]
                current_length = sum(len(line)+1 for line in current_page)
            
            current_page.append(line)
            current_length += line_length

        # å¤„ç†å‰©ä½™å†…å®¹
        if current_page:
            # å¼ºåˆ¶ç¡®ä¿æœ€åä¸€è¡Œä¸ºm3u8
            last_m3u8 = None
            for i in reversed(range(len(current_page))):
                if 'm3u8' in current_page[i].lower():
                    last_m3u8 = i + 1
                    break
            
            if last_m3u8 is not None:
                valid_page = current_page[:last_m3u8]
                remaining = current_page[last_m3u8:]
            else:
                valid_page = current_page
                remaining = []

            if valid_page:
                pages.append('\n'.join(header + valid_page + footer))
            if remaining:
                pages.append('\n'.join(header + remaining + footer))

        return pages or ['\n'.join(header + footer)]

    async def _common_handler(self, event, api_urls, keyword):
        """æ ¸å¿ƒå¤„ç†é€»è¾‘"""
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}
        ordered_titles = []
        
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            encoded_keyword = urllib.parse.quote(keyword)
            query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue

                        html_content = await response.text()
                        parsed_items = self._parse_html(html_content)
                        
                        if parsed_items:
                            successful_apis += 1
                            for title, url in parsed_items:
                                if title not in grouped_results:
                                    grouped_results[title] = []
                                    ordered_titles.append(title)
                                grouped_results[title].append(url)

            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue

        # æ„å»ºç»“æœ
        result_lines = []
        total_videos = sum(len(urls) for urls in grouped_results.values())
        
        for idx, title in enumerate(ordered_titles, 1):
            urls = grouped_results.get(title, [])
            result_lines.append(f"{idx}. ã€{title}ã€‘")
            result_lines.extend([f"   ğŸ¬ {url}" for url in urls])

        # åˆ†é¡µå¤„ç†
        header = [
            f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
            f"ğŸ“Š æ‰¾åˆ° {total_videos} æ¡èµ„æº",
            "â”" * 30
        ]
        footer = [
            "â”" * 30,
            "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
            "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
            "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
            "â”" * 30
        ]
        
        return self._split_into_pages(header, result_lines, footer) if result_lines else [
            f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº"
        ]

    def _parse_html(self, html_content):
        """è§£æHTMLå†…å®¹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        video_items = soup.select('rss list video')[:self.records]
        
        parsed_data = []
        for item in video_items:
            title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
            for dd in item.select('dl > dd'):
                for url in dd.text.split('#'):
                    if url := url.strip():
                        parsed_data.append((title, url))
        return parsed_data

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šå½±è§†æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        
        pages = await self._common_handler(event, self.api_url_vod, text)
        session_id = self._get_session_id(event)
        
        self.pagination_cache[session_id] = {
            "pages": pages,
            "timestamp": time.time()
        }
        
        yield event.plain_result(pages[0])
        if len(pages) > 1:
            yield event.plain_result(f"ã€åˆ†é¡µæç¤ºã€‘å›å¤2-{len(pages)}æŸ¥çœ‹åç»­å†…å®¹ï¼ˆ20ç§’å†…æœ‰æ•ˆï¼‰")

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """æˆäººå†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("ğŸ” æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        
        pages = await self._common_handler(event, self.api_url_18, text)
        session_id = self._get_session_id(event)
        
        self.pagination_cache[session_id] = {
            "pages": pages,
            "timestamp": time.time()
        }
        
        yield event.plain_result(pages[0])
        if len(pages) > 1:
            yield event.plain_result(f"ã€åˆ†é¡µæç¤ºã€‘å›å¤2-{len(pages)}æŸ¥çœ‹åç»­å†…å®¹ï¼ˆ20ç§’å†…æœ‰æ•ˆï¼‰")

    @filter.regex(r"^\d+$")
    async def handle_pagination(self, event: AstrMessageEvent):
        """å¤„ç†åˆ†é¡µè¯·æ±‚"""
        session_id = self._get_session_id(event)
        cache = self.pagination_cache.get(session_id)
        
        # ç¼“å­˜æœ‰æ•ˆæ€§éªŒè¯
        if not cache or (time.time() - cache["timestamp"]) > 20:
            if session_id in self.pagination_cache:
                del self.pagination_cache[session_id]
            yield event.plain_result("â³ åˆ†é¡µå·²è¿‡æœŸï¼Œè¯·é‡æ–°æœç´¢")
            return
        
        try:
            page_num = int(event.message_str.strip())
        except ValueError:
            return
        
        pages = cache["pages"]
        if 1 < page_num <= len(pages):
            # æ›´æ–°ç¼“å­˜æ—¶é—´
            self.pagination_cache[session_id]["timestamp"] = time.time()
            yield event.plain_result(pages[page_num - 1])
        elif page_num == 1:
            yield event.plain_result("ğŸ“– å·²ç»æ˜¯ç¬¬ä¸€é¡µå•¦")
        else:
            yield event.plain_result(f"âŒ æ— æ•ˆé¡µç ï¼Œè¯·è¾“å…¥2-{len(pages)}ä¹‹é—´çš„æ•°å­—")
