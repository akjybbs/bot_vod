from typing import Dict
from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api.types import EventMessageType
import aiohttp
import urllib.parse
import asyncio
import time
from bs4 import BeautifulSoup

# åˆ†é¡µçŠ¶æ€å­˜å‚¨ç»“æ„
VIDEO_PAGES: Dict[int, Dict] = {}

@register("bot_vod", "appale", "åˆ†é¡µå½±è§†æœç´¢ï¼ˆ/vod ç”µå½±åï¼‰", "2.0")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))
        self.page_timeout = 20  # åˆ†é¡µè¶…æ—¶æ—¶é—´

    async def _common_handler(self, event, api_urls, keyword):
        """å¸¦åˆ†é¡µçš„è¯·æ±‚å¤„ç†æ ¸å¿ƒæ–¹æ³•"""
        # åŸå§‹APIè¯·æ±‚é€»è¾‘
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}
        ordered_titles = []
        
        # éå†æ‰€æœ‰APIæº
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            encoded_keyword = urllib.parse.quote(keyword)
            query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue

                        html_content = await response.text()
                        parsed_items = self._parse_html(html_content)
                        
                        if parsed_items:
                            successful_apis += 1
                            # åˆå¹¶ç»“æœå¹¶ä¿æŒé¡ºåº
                            for title, url in parsed_items:
                                if title not in grouped_results:
                                    grouped_results[title] = []
                                    ordered_titles.append(title)
                                grouped_results[title].append(url)

            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue

        # æ„å»ºç»“æœåˆ—è¡¨
        result_lines = []
        for idx, title in enumerate(ordered_titles, 1):
            urls = grouped_results.get(title, [])
            result_lines.append(f"{idx}. ã€{title}ã€‘")
            result_lines.extend([f"   ğŸ¬ {url}" for url in urls])

        # ç”Ÿæˆåˆ†é¡µå†…å®¹
        pages = self._build_pages(
            total_attempts=total_attempts,
            successful_apis=successful_apis,
            total_results=sum(len(urls) for urls in grouped_results.values()),
            result_lines=result_lines
        )

        if not pages:
            yield event.plain_result("æœªæ‰¾åˆ°ç›¸å…³èµ„æº")
            return

        # å­˜å‚¨åˆ†é¡µçŠ¶æ€
        user_id = event.get_sender_id()
        VIDEO_PAGES[user_id] = {
            "pages": pages,
            "timestamp": time.time(),
            "total_pages": len(pages)
        }

        # å‘é€ç¬¬ä¸€é¡µ
        yield event.plain_result(pages[0])

        # è®¾ç½®è¶…æ—¶æ¸…ç†
        self._schedule_cleanup(user_id)

    def _build_pages(self, total_attempts: int, successful_apis: int, total_results: int, result_lines: list) -> list:
        """æ™ºèƒ½åˆ†é¡µæ„å»ºå™¨"""
        MAX_PAGE_LENGTH = 900  # ç•™å‡ºå¾®ä¿¡æ¶ˆæ¯ä½™é‡
        pages = []
        current_page = []
        current_length = 0
        
        # æ„å»ºé¡µå¤´
        header = [
            f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
            f"ğŸ“Š æ‰¾åˆ° {total_results} æ¡èµ„æº",
            "â”" * 30
        ]
        header_length = sum(len(line)+1 for line in header)
        
        # æ„å»ºé¡µè„š
        footer = [
            "â”" * 30,
            "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
            "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
            "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
            "â”" * 30,
            f"ğŸ“„ å›å¤é¡µç æŸ¥çœ‹åç»­å†…å®¹ï¼ˆ{self.page_timeout}ç§’å†…æœ‰æ•ˆï¼‰"
        ]
        footer_length = sum(len(line)+1 for line in footer) + 10  # é¡µç æç¤ºä½™é‡

        # åˆå§‹é¡µ
        current_page.extend(header)
        current_length = header_length
        page_num = 1

        for line in result_lines:
            line_length = len(line) + 1  # æ¢è¡Œç¬¦å 1å­—ç¬¦

            # å¼ºåˆ¶åˆ†é¡µæ¡ä»¶ï¼šé‡åˆ°m3u8é“¾æ¥
            if ".m3u8" in line:
                if current_page and current_page[-1].startswith("ğŸ“„"):
                    current_page.pop()  # ç§»é™¤æ—§é¡µç æç¤º
                current_page.append(f"ğŸ“„ å½“å‰ç¬¬ {page_num} é¡µ")
                full_page = "\n".join(current_page + footer)
                pages.append(full_page)
                
                # é‡ç½®é¡µé¢
                page_num += 1
                current_page = header.copy()
                current_length = header_length
                continue

            # å¸¸è§„åˆ†é¡µæ£€æŸ¥
            if current_length + line_length + footer_length > MAX_PAGE_LENGTH:
                current_page.append(f"ğŸ“„ å½“å‰ç¬¬ {page_num} é¡µ")
                full_page = "\n".join(current_page + footer)
                pages.append(full_page)
                
                # é‡ç½®é¡µé¢
                page_num += 1
                current_page = header.copy()
                current_length = header_length

            # æ·»åŠ å†…å®¹
            current_page.append(line)
            current_length += line_length

        # å¤„ç†æœ€åä¸€é¡µ
        if len(current_page) > len(header):
            current_page.append(f"ğŸ“„ å½“å‰ç¬¬ {page_num} é¡µ")
            full_page = "\n".join(current_page + footer)
            pages.append(full_page)

        return pages

    def _schedule_cleanup(self, user_id: int):
        """è®¡åˆ’ä»»åŠ¡æ¸…ç†è¿‡æœŸçŠ¶æ€"""
        loop = asyncio.get_running_loop()
        loop.call_later(self.page_timeout, self._cleanup_page_state, user_id)

    def _cleanup_page_state(self, user_id: int):
        """å®é™…æ¸…ç†çŠ¶æ€"""
        if user_id in VIDEO_PAGES:
            if time.time() - VIDEO_PAGES[user_id]["timestamp"] > self.page_timeout:
                del VIDEO_PAGES[user_id]
                self.context.logger.debug(f"å·²æ¸…ç†ç”¨æˆ· {user_id} çš„åˆ†é¡µçŠ¶æ€")

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šå½±è§†æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_vod, text):
            yield msg

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """ğŸ”å†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("ğŸ”æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_18, text):
            yield msg

    @filter.event_message_type(EventMessageType.TEXT)
    async def handle_page_request(self, event: AstrMessageEvent):
        """å¤„ç†åˆ†é¡µè¯·æ±‚"""
        user_id = event.get_sender_id()
        message = event.message_str.strip()

        # éªŒè¯çŠ¶æ€å­˜åœ¨æ€§
        if user_id not in VIDEO_PAGES:
            return

        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å­—
        if not message.isdigit():
            return

        page_num = int(message)
        page_data = VIDEO_PAGES[user_id]

        # éªŒè¯é¡µç èŒƒå›´
        if 1 <= page_num <= page_data["total_pages"]:
            # æ›´æ–°çŠ¶æ€æ—¶é—´æˆ³
            VIDEO_PAGES[user_id]["timestamp"] = time.time()

            # å‘é€è¯·æ±‚é¡µ
            yield event.plain_result(page_data["pages"][page_num-1])

            # é‡ç½®è¶…æ—¶è®¡æ—¶
            self._schedule_cleanup(user_id)

    def _parse_html(self, html_content: str) -> list:
        """HTMLè§£æå™¨"""
        soup = BeautifulSoup(html_content, 'html.parser')
        video_items = soup.select('rss list video')[:self.records]
        
        parsed_data = []
        for item in video_items:
            title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
            # æå–æ‰€æœ‰æ’­æ”¾é“¾æ¥
            for dd in item.select('dl > dd'):
                for url in dd.text.split('#'):
                    if url := url.strip():
                        parsed_data.append((title, url))
        return parsed_data
