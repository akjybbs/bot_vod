from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import urllib.parse
from bs4 import BeautifulSoup
import time
import asyncio
import re

@register("bot_vod", "appale", "è§†é¢‘æœç´¢åŠåˆ†é¡µåŠŸèƒ½ï¼ˆå‘½ä»¤ï¼š/vod /vodd /ç¿»é¡µï¼‰", "2.0.6")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))
        self.user_pages = {}

    def _get_user_identity(self, event: AstrMessageEvent) -> str:
        """ç”¨æˆ·æ ‡è¯†è·å–ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            if hasattr(event, 'get_sender_id'):
                return f"{event.platform}-{event.get_sender_id()}"
            return f"{event.platform}-{hash(event)}"
        except Exception as e:
            self.context.logger.error(f"æ ‡è¯†è·å–å¼‚å¸¸: {str(e)}")
            return "unknown_user"

    async def _common_handler(self, event: AstrMessageEvent, api_urls: list, keyword: str):
        """æ ¸å¿ƒæœç´¢é€»è¾‘ï¼ˆç»ˆæåˆ†é¡µä¼˜åŒ–ï¼‰"""
        user_id = self._get_user_identity(event)
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}
        ordered_titles = []

        # APIè¯·æ±‚å¤„ç†ï¼ˆä¿æŒä¸å˜ï¼‰
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            try:
                encoded_keyword = urllib.parse.quote(keyword)
                query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue
                            
                        html_content = await response.text()
                        soup = BeautifulSoup(html_content, 'html.parser')
                        video_items = soup.select('rss list video')[:self.records]
                        
                        for item in video_items:
                            title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
                            for dd in item.select('dl > dd'):
                                for url in dd.text.split('#'):
                                    if (url := url.strip()):
                                        if title not in grouped_results:
                                            grouped_results[title] = []
                                            ordered_titles.append(title)
                                        grouped_results[title].append({
                                            "url": url,
                                            "is_m3u8": url.endswith('.m3u8')
                                        })
                        successful_apis += 1
                        
            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚é”™è¯¯: {str(e)}")
                continue

        # æ„å»ºç»“æ„åŒ–ç»“æœ
        structured_results = []
        for idx, title in enumerate(ordered_titles, 1):
            entries = grouped_results.get(title, [])
            structured_results.append({
                "title": f"{idx}. ã€{title}ã€‘",
                "urls": entries
            })

        # æ™ºèƒ½åˆ†é¡µå¤„ç†ï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆï¼‰
        pages = []
        if structured_results:
            header = [
                f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
                f"ğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº",
                "â”" * 28
            ]
            
            expiry_time = time.strftime("%H:%M", time.gmtime(time.time() + 300 + 8*3600))
            
            # åŠ¨æ€åˆ†é¡µå‚æ•°
            MAX_PAGE_SIZE = 1000    # å­—ç¬¦æ•°é™åˆ¶
            current_page = []
            current_size = len('\n'.join(header))
            pending_blocks = []
            merge_threshold = 500   # åˆå¹¶é˜ˆå€¼

            def commit_page():
                """æäº¤å½“å‰é¡µå¹¶æ¸…ç©ºç¼“å­˜"""
                nonlocal current_page, current_size
                if current_page:
                    pages.append('\n'.join(header + current_page))
                    current_page.clear()
                    current_size = len('\n'.join(header))

            for block_idx, block in enumerate(structured_results):
                block_lines = [block["title"]] + [f"   ğŸ¬ {u['url']}" for u in block["urls"]]
                block_content = '\n'.join(block_lines)
                block_size = len(block_content)
                
                # æ™ºèƒ½åˆå¹¶ç­–ç•¥
                if current_size + block_size > MAX_PAGE_SIZE:
                    # å½“å‰é¡µé¢å‰©ä½™ç©ºé—´è¶³å¤Ÿåˆå¹¶ç§¯å‹å—
                    if pending_blocks and (current_size + sum(len(b) for b in pending_blocks) <= MAX_PAGE_SIZE):
                        current_page.extend(pending_blocks)
                        current_size += sum(len(b) for b in pending_blocks) + len(pending_blocks)
                        pending_blocks.clear()
                        commit_page()
                    
                    # å¤„ç†è¶…å¤§å—ï¼ˆè¶…è¿‡é¡µé¢70%ï¼‰
                    if block_size > MAX_PAGE_SIZE * 0.7:
                        commit_page()
                        current_page = block_lines
                        commit_page()
                    else:
                        pending_blocks.extend(block_lines)
                else:
                    # é¢„æµ‹å‰©ä½™ç©ºé—´
                    remaining = MAX_PAGE_SIZE - (current_size + block_size)
                    if remaining < merge_threshold:
                        pending_blocks.extend(block_lines)
                    else:
                        current_page.extend(block_lines)
                        current_size += block_size + 1  # +1ä¸ºæ¢è¡Œç¬¦

            # æœ€ç»ˆå¤„ç†ç§¯å‹å—
            if pending_blocks:
                # ä¼˜å…ˆå°è¯•åˆå¹¶åˆ°å½“å‰é¡µ
                pending_size = sum(len(line) for line in pending_blocks) + len(pending_blocks)
                if current_size + pending_size <= MAX_PAGE_SIZE:
                    current_page.extend(pending_blocks)
                    commit_page()
                else:
                    # åˆ†å‰²ç§¯å‹å—
                    temp_page = []
                    temp_size = current_size
                    for line in pending_blocks:
                        line_size = len(line) + 1
                        if temp_size + line_size > MAX_PAGE_SIZE:
                            current_page.extend(temp_page)
                            commit_page()
                            temp_page = [line]
                            temp_size = len('\n'.join(header)) + line_size
                        else:
                            temp_page.append(line)
                            temp_size += line_size
                    current_page.extend(temp_page)
                    commit_page()

            # æ·»åŠ ç»Ÿä¸€é¡µè„š
            total_pages = len(pages)
            for page_num in range(total_pages):
                pages[page_num] = self._build_page_footer(
                    content=pages[page_num],
                    page_num=page_num+1,
                    total_pages=total_pages,
                    expiry_time=expiry_time
                )

            # å­˜å‚¨åˆ†é¡µæ•°æ®
            self.user_pages[user_id] = {
                "pages": pages,
                "timestamp": time.time(),
                "total_pages": total_pages,
                "search_info": f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\nğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº"
            }
            yield event.plain_result(pages[0])
        else:
            yield event.plain_result(f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº")

    def _build_page_footer(self, content: str, page_num: int, total_pages: int, expiry_time: str) -> str:
        """æ„å»ºå®Œæ•´é¡µè„šï¼ˆç²¾ç¡®æ—¶é—´å¤„ç†ï¼‰"""
        footer = [
            "â”" * 28,
            f"ğŸ“‘ ç¬¬ {page_num}/{total_pages} é¡µ",
            f"â° æœ‰æ•ˆæœŸè‡³ {expiry_time}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰",
            "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
            "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
            "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
            "3. ä½¿ç”¨:/ç¿»é¡µ é¡µç (è·³è½¬é¡µé¢)",
            "â”" * 28
        ]
        return re.sub(r'â° æœ‰æ•ˆæœŸè‡³ .*?\n', '\n'.join(footer[2:4]) + '\n', content, count=1)

    @filter.command("ç¿»é¡µ")
    async def paginate_results(self, event: AstrMessageEvent, text: str):
        """åˆ†é¡µæŸ¥çœ‹ç»“æœï¼ˆç²¾ç¡®æ—¶é—´æ›¿æ¢ï¼‰"""
        user_id = self._get_user_identity(event)
        page_data = self.user_pages.get(user_id)

        if not page_data or (time.time() - page_data["timestamp"]) > 300:
            yield event.plain_result("â³ æœç´¢ç»“æœå·²è¿‡æœŸï¼ˆæœ‰æ•ˆæœŸ5åˆ†é’Ÿï¼‰ï¼Œè¯·é‡æ–°æœç´¢")
            return

        try:
            page_num = int(text.strip())
            if not 1 <= page_num <= page_data["total_pages"]:
                raise ValueError
        except ValueError:
            yield event.plain_result(f"âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé¡µç ï¼ˆ1-{page_data['total_pages']}ï¼‰")
            return

        # ç²¾ç¡®æ—¶é—´æ›¿æ¢
        new_expiry = time.strftime("%H:%M", time.gmtime(time.time() + 300 + 8*3600))
        pattern = r'(â° æœ‰æ•ˆæœŸè‡³ )\d{2}:\d{2}'
        content = re.sub(pattern, f'\\g<1>{new_expiry}', page_data["pages"][page_num-1], count=1)
        yield event.plain_result(content)

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
