from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import urllib.parse
from bs4 import BeautifulSoup
import time
import asyncio
import re

@register("bot_vod", "appale", "è§†é¢‘æœç´¢åŠåˆ†é¡µåŠŸèƒ½ï¼ˆå‘½ä»¤ï¼š/vod /vodd /ç¿»é¡µï¼‰", "2.1.0")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))
        self.user_pages = {}
        self.MAX_PAGE_LENGTH = 980  # é¢„ç•™é¡µè„šç©ºé—´
        self.MIN_CONTENT_LINES = 4  # æ ‡é¢˜+è‡³å°‘3ä¸ªURL

    def _get_user_identity(self, event: AstrMessageEvent) -> str:
        """å¢å¼ºç”¨æˆ·æ ‡è¯†è·å–"""
        try:
            if hasattr(event, 'get_sender_id'):
                return f"{event.platform}-{event.get_sender_id()}"
            elif hasattr(event.user, 'openid'):
                return f"wechat-{event.user.openid}"
            return f"fallback-{hash(event)}"
        except Exception as e:
            self.context.logger.error(f"æ ‡è¯†è·å–å¤±è´¥: {str(e)}")
            return "unknown_user"

    async def _common_handler(self, event: AstrMessageEvent, api_urls: list, keyword: str):
        """ä¼˜åŒ–ç‰ˆåˆ†é¡µæ ¸å¿ƒé€»è¾‘"""
        user_id = self._get_user_identity(event)
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}
        ordered_titles = []

        # APIè¯·æ±‚å¤„ç†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            try:
                encoded_keyword = urllib.parse.quote(keyword)
                query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue
                            
                        html_content = await response.text()
                        soup = BeautifulSoup(html_content, 'html.parser')
                        video_items = soup.select('rss list video')[:self.records]
                        
                        for item in video_items:
                            title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
                            for dd in item.select('dl > dd'):
                                for url in dd.text.split('#'):
                                    if (url := url.strip()):
                                        if title not in grouped_results:
                                            grouped_results[title] = []
                                            ordered_titles.append(title)
                                        grouped_results[title].append({
                                            "url": url,
                                            "is_m3u8": url.endswith('.m3u8')
                                        })
                        successful_apis += 1
                        
            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue

        # æ„å»ºç»“æ„åŒ–ç»“æœ
        structured_results = []
        for idx, title in enumerate(ordered_titles, 1):
            entries = grouped_results.get(title, [])
            structured_results.append({
                "title": f"{idx}. ã€{title}ã€‘",
                "urls": entries
            })

        # æ™ºèƒ½åˆ†é¡µå¤„ç†å™¨
        pages = []
        if structured_results:
            header = [
                f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
                f"ğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº",
                "â”" * 28
            ]
            footer_base = [
                "â”" * 28,
                "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
                "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
                "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
                "3. ä½¿ç”¨:/ç¿»é¡µ é¡µç (è·³è½¬é¡µé¢)",
                "â”" * 28
            ]

            # æœ‰æ•ˆæœŸè®¡ç®—
            expiry_timestamp = time.time() + 300
            beijing_time = time.strftime("%H:%M", time.gmtime(expiry_timestamp + 8 * 3600))
            time_footer = [
                f"â° æœ‰æ•ˆæœŸè‡³ {beijing_time}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰",
                *footer_base
            ]

            # å•æ ‡é¢˜å¤„ç†
            if len(structured_results) == 1:
                page_content = [header[0], header[1], header[2]]
                title_block = structured_results[0]
                page_content.append(title_block["title"])
                for url_info in title_block["urls"]:
                    page_content.append(f"   ğŸ¬ {url_info['url']}")
                page_content.extend(time_footer)
                pages.append('\n'.join(page_content))
            else:
                # å¤šæ ‡é¢˜åˆ†é¡µé€»è¾‘ï¼ˆä¼˜åŒ–æ ¸å¿ƒï¼‰
                current_page = []
                current_length = len('\n'.join(header)) + 1  # åŒ…å«æ¢è¡Œç¬¦
                last_m3u8_index = -1

                def finalize_page():
                    nonlocal current_page, last_m3u8_index
                    if not current_page:
                        return False

                    # æ™ºèƒ½åˆ†å‰²ç‚¹é€‰æ‹©
                    split_index = len(current_page)
                    if last_m3u8_index != -1 and last_m3u8_index < len(current_page)-1:
                        split_index = last_m3u8_index + 1
                    elif len(current_page) >= self.MIN_CONTENT_LINES:
                        # æŸ¥æ‰¾æœ€è¿‘çš„æ ‡é¢˜åˆ†å‰²ç‚¹
                        for i in reversed(range(len(current_page))):
                            if re.match(r"^\d+\. ã€", current_page[i]):
                                split_index = i
                                break

                    final_content = current_page[:split_index]
                    remaining_content = current_page[split_index:]

                    # å‰©ä½™å†…å®¹æœ‰æ•ˆæ€§æ£€æŸ¥
                    if len(remaining_content) < (self.MIN_CONTENT_LINES // 2):
                        final_content = current_page
                        remaining_content = []

                    # æ„å»ºé¡µè„š
                    page_footer = [
                        "â”" * 28,
                        f"ğŸ“‘ ç¬¬ {len(pages)+1}/PAGES é¡µ",
                        *time_footer
                    ]
                    
                    full_content = '\n'.join(header + final_content + page_footer)
                    pages.append(full_content)
                    
                    # æ›´æ–°çŠ¶æ€
                    current_page = remaining_content
                    if current_page:
                        current_length = len('\n'.join(header)) + len('\n'.join(current_page)) + 1
                        last_m3u8_index = -1
                        return True
                    return False

                for title_block in structured_results:
                    title_line = title_block["title"]
                    url_lines = [f"   ğŸ¬ {u['url']}" for u in title_block["urls"]]
                    
                    # é¢„åˆ¤æ•´ä¸ªå—çš„é•¿åº¦
                    block_content = [title_line] + url_lines
                    block_length = len('\n'.join(block_content))
                    
                    # å—çº§åˆ†é¡µå†³ç­–
                    if current_length + block_length > self.MAX_PAGE_LENGTH:
                        while finalize_page():
                            pass
                    
                    # æ·»åŠ æ ‡é¢˜è¡Œ
                    current_page.append(title_line)
                    current_length += len(title_line) + 1
                    
                    # æ‰¹é‡æ·»åŠ URLï¼ˆä¼˜åŒ–ç¢ç‰‡ï¼‰
                    url_bulk = []
                    bulk_length = 0
                    for i, url_line in enumerate(url_lines):
                        line_length = len(url_line) + 1
                        if title_block["urls"][i]["is_m3u8"]:
                            last_m3u8_index = len(current_page) + len(url_bulk)
                        
                        # æ‰¹é‡æäº¤é€»è¾‘
                        if bulk_length + line_length < 200:  # æ‰¹é‡é˜ˆå€¼
                            url_bulk.append(url_line)
                            bulk_length += line_length
                        else:
                            current_page.extend(url_bulk)
                            current_length += bulk_length
                            url_bulk = [url_line]
                            bulk_length = line_length
                        
                        # å¼ºåˆ¶åˆ†é¡µæ£€æŸ¥
                        if current_length + bulk_length > self.MAX_PAGE_LENGTH:
                            current_page.extend(url_bulk)
                            current_length += bulk_length
                            url_bulk = []
                            bulk_length = 0
                            if finalize_page():
                                current_length = len('\n'.join(header)) + len('\n'.join(current_page)) + 1
                    
                    # æäº¤å‰©ä½™æ‰¹é‡
                    if url_bulk:
                        current_page.extend(url_bulk)
                        current_length += bulk_length
                
                # æœ€ç»ˆå†…å®¹å¤„ç†
                while len(current_page) >= (self.MIN_CONTENT_LINES // 2):
                    if not finalize_page():
                        break

            # æ›´æ–°æ€»é¡µæ•°
            total_pages = len(pages)
            for i in range(len(pages)):
                pages[i] = pages[i].replace("PAGES", str(total_pages))
                
            # å­˜å‚¨åˆ†é¡µçŠ¶æ€
            self.user_pages[user_id] = {
                "pages": pages,
                "timestamp": time.time(),
                "total_pages": total_pages,
                "search_info": f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\nğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº"
            }
            yield event.plain_result(pages[0])
        else:
            yield event.plain_result(f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº")

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šè§†é¢‘æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_vod, text):
            yield msg

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """æˆäººå†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("ğŸ” æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_18, text):
            yield msg

    @filter.command("ç¿»é¡µ")
    async def paginate_results(self, event: AstrMessageEvent, text: str):
        """æ™ºèƒ½åˆ†é¡µæ§åˆ¶"""
        user_id = self._get_user_identity(event)
        page_data = self.user_pages.get(user_id)

        # æœ‰æ•ˆæ€§éªŒè¯
        if not page_data or (time.time() - page_data["timestamp"]) > 300:
            yield event.plain_result("â³ æœç´¢ç»“æœå·²è¿‡æœŸï¼ˆæœ‰æ•ˆæœŸ5åˆ†é’Ÿï¼‰ï¼Œè¯·é‡æ–°æœç´¢")
            return

        # é¡µç å¤„ç†
        try:
            input_page = text.strip()
            if input_page.startswith('ç¬¬') and input_page.endswith('é¡µ'):
                page_num = int(input_page[1:-1])
            else:
                page_num = int(input_page)
            
            if not 1 <= page_num <= page_data["total_pages"]:
                raise ValueError
        except (ValueError, IndexError):
            yield event.plain_result(f"âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé¡µç ï¼ˆ1-{page_data['total_pages']}ï¼‰")
            return

        # åŠ¨æ€æ›´æ–°æœ‰æ•ˆæœŸ
        page_data['timestamp'] = time.time()
        expiry_timestamp = page_data['timestamp'] + 300
        beijing_time = time.strftime("%H:%M", time.gmtime(expiry_timestamp + 8 * 3600))
        
        # ä½¿ç”¨æ­£åˆ™æ›´æ–°æ—¶æ•ˆæ˜¾ç¤º
        updated_page = re.sub(
            r"â° æœ‰æ•ˆæœŸè‡³ \d{2}:\d{2}",
            f"â° æœ‰æ•ˆæœŸè‡³ {beijing_time}",
            page_data["pages"][page_num-1]
        )
        yield event.plain_result(updated_page)
