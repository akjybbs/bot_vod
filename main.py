from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
from typing import AsyncGenerator

MAX_RESULT_LINES = 25  # ç»“æœæœ€å¤§è¡Œæ•°æ§åˆ¶

@register("bot_vod", "appale", "ä»APIè·å–è§†é¢‘åœ°å€ï¼ˆä½¿ç”¨ /vod æˆ– /vodd + ç”µå½±åï¼‰", "1.2")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self._init_config(config)
        self.session = None  # å»¶è¿Ÿåˆ›å»ºSession

    def _init_config(self, config):
        """é…ç½®åˆå§‹åŒ–ä¸æ ¡éªŒ"""
        # APIæºå¤„ç†
        self.api_url_vod = [url.strip() for url in config.get("api_url_vod", "").split(',') if url.strip()]
        self.api_url_18 = [url.strip() for url in config.get("api_url_18", "").split(',') if url.strip()]
        
        # ç»“æœæ•°é‡æ§åˆ¶
        self.records = max(1, int(config.get("records", 3)))
        
        # è¶…æ—¶è®¾ç½®
        self.timeout = aiohttp.ClientTimeout(total=20)

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†"""
        self.session = aiohttp.ClientSession(timeout=self.timeout)
        return self

    async def __aexit__(self, exc_type, exc, tb):
        """ç¡®ä¿å…³é—­Session"""
        if self.session:
            await self.session.close()

    @filter.command("vod", "vodd")
    async def unified_search(self, event: AstrMessageEvent, text: str) -> AsyncGenerator[MessageEventResult, None]:
        """ç»Ÿä¸€æœç´¢å…¥å£"""
        is_adult = event.command == "vodd"
        api_config = self.api_url_18 if is_adult else self.api_url_vod
        service_type = "æˆäºº" if is_adult else "æ™®é€š"
        
        if not api_config:
            yield event.plain_result(f"âš ï¸ {service_type}è§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        if not text:
            yield event.plain_result("ğŸ” è¯·è¾“å…¥æœç´¢å†…å®¹ï¼ˆç¤ºä¾‹ï¼š/vod æµæµªåœ°çƒï¼‰")
            return

        async for msg in self._process_search(event, api_config, text.strip(), len(api_config)):
            yield msg

    async def _process_search(self, event, api_urls, keyword, total_apis):
        """æœç´¢å¤„ç†æµç¨‹"""
        try:
            # å¹¶å‘è¯·æ±‚æ‰€æœ‰API
            tasks = [self._fetch_api_data(url, keyword) for url in api_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»“æœèšåˆ
            grouped_data = self._aggregate_results(results)
            if not grouped_data:
                yield event.plain_result(self._build_empty_result(total_apis, 0))
                return
                
            # æ¶ˆæ¯æ„å»º
            success_apis = sum(1 for r in results if not isinstance(r, Exception) and r)
            message = self._construct_message(
                total_apis=total_apis,
                success_apis=success_apis,
                grouped_data=grouped_data
            )
            yield event.plain_result(message)
            
        except Exception as e:
            self.context.logger.error(f"æœç´¢å¼‚å¸¸: {str(e)}", exc_info=True)
            yield event.plain_result("âš ï¸ æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")

    async def _fetch_api_data(self, api_url, keyword):
        """æ‰§è¡ŒAPIè¯·æ±‚"""
        try:
            encoded_keyword = urllib.parse.quote(keyword)
            async with self.session.get(
                f"{api_url}?ac=videolist&wd={encoded_keyword}",
                allow_redirects=False
            ) as resp:
                if resp.status == 200:
                    return await resp.text()
                return None
        except Exception as e:
            self.context.logger.debug(f"APIè¯·æ±‚å¤±è´¥ [{api_url}]: {str(e)}")
            return e  # è¿”å›å¼‚å¸¸ç”¨äºç»Ÿè®¡

    def _aggregate_results(self, results):
        """èšåˆå¤šAPIç»“æœ"""
        grouped = {}
        ordered_titles = []
        
        for html in results:
            if isinstance(html, Exception) or not html:
                continue
                
            for title, url in self._parse_html(html):
                if title not in grouped:
                    grouped[title] = []
                    ordered_titles.append(title)
                grouped[title].append(url)
        
        return {
            "grouped": grouped,
            "ordered_titles": ordered_titles,
            "total": sum(len(urls) for urls in grouped.values())
        }

    def _parse_html(self, html):
        """è§£æHTMLå†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            items = soup.select('rss list video')[:self.records]
            
            for item in items:
                title_elem = item.select_one('name')
                title = title_elem.text.strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
                
                # æå–æ‰€æœ‰æœ‰æ•ˆURL
                for dd in item.select('dl > dd'):
                    for url in dd.text.split('#'):
                        if url := url.strip():
                            yield (title, url)
        except Exception as e:
            self.context.logger.warning(f"è§£æå¼‚å¸¸: {str(e)}")
            return []

    def _construct_message(self, total_apis, success_apis, grouped_data):
        """æ„å»ºå®Œæ•´æ¶ˆæ¯"""
        lines = []
        
        # æ·»åŠ æ ‡é¢˜
        lines.extend(self._build_header(total_apis, success_apis, grouped_data["total"]))
        
        # å¡«å……å†…å®¹
        line_count = len(lines)
        for idx, title in enumerate(grouped_data["ordered_titles"], 1):
            title_line = f"{idx}. ã€{title}ã€‘"
            url_lines = [f"   ğŸ¬ {url}" for url in grouped_data["grouped"][title][:3]]  # æ¯ä¸ªèµ„æºæœ€å¤šæ˜¾ç¤º3æ¡
            
            # è¡Œæ•°æ§åˆ¶
            if line_count + len(url_lines) + 1 > MAX_RESULT_LINES:
                lines.append("...ï¼ˆç»“æœå·²æˆªæ–­ï¼‰")
                break
                
            lines.append(title_line)
            lines.extend(url_lines)
            line_count += len(url_lines) + 1
        
        # æ·»åŠ å°¾éƒ¨
        lines.extend(self._build_footer())
        return "\n".join(lines)

    def _build_header(self, total_apis, success_apis, total_videos):
        """æ¶ˆæ¯å¤´éƒ¨æ¨¡æ¿"""
        return [
            f"ğŸ” æœç´¢ {total_apis} ä¸ªæºï½œæˆåŠŸ {success_apis} ä¸ª",
            f"ğŸ“Š æ‰¾åˆ° {total_videos} æ¡èµ„æº",
            "â”" * 26
        ]

    def _build_footer(self):
        """æ¶ˆæ¯å°¾éƒ¨æ¨¡æ¿"""
        return [
            "â”" * 26,
            "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
            "â€¢ ç§»åŠ¨ç«¯ï¼šç›´æ¥å¤åˆ¶é“¾æ¥åˆ°æµè§ˆå™¨",
            "â€¢ ç”µè„‘ç«¯ï¼šæ¨èä½¿ç”¨PotPlayer/VLC",
            "â”" * 26
        ]

    def _build_empty_result(self, total_apis, success_apis):
        """æ— ç»“æœæ¶ˆæ¯"""
        return (
            f"ğŸ” æœç´¢ {total_apis} ä¸ªæºï½œæˆåŠŸ {success_apis} ä¸ª\n"
            f"{'â”'*26}\n"
            "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æº\n"
            f"{'â”'*26}"
        )
