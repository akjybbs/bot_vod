from typing import Dict
from astrbot.api.all import *
from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import urllib.parse
from bs4 import BeautifulSoup
import time
from typing import Dict

# ç”¨æˆ·çŠ¶æ€è·Ÿè¸ªï¼Œè®°å½•åˆ†é¡µä¿¡æ¯å’Œæ—¶é—´æˆ³
USER_STATES: Dict[str, Dict] = {}

@register("bot_vod", "appale", "ä»APIè·å–è§†é¢‘åœ°å€ï¼ˆä½¿ç”¨ /vod æˆ– /vodd + ç”µå½±åï¼‰", "1.1")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))

    def _split_into_pages(self, lines: list) -> list:
        """å°†ç»“æœåˆ†é¡µï¼Œæ¯é¡µæœ€åä¸€è¡Œä»¥m3u8ç»“å°¾ä¸”ä¸è¶…è¿‡1000å­—ç¬¦"""
        pages = []
        current_page = []
        current_length = 0
        last_m3u8 = -1

        for line in lines:
            line_len = len(line) + 1  # åŒ…å«æ¢è¡Œç¬¦
            # é¢„åˆ¤æ·»åŠ åæ˜¯å¦è¶…é™
            if current_length + line_len > 1000:
                if last_m3u8 != -1:
                    # åˆ‡å‰²åˆ°æœ€åä¸€ä¸ªm3u8ä½ç½®
                    valid_page = current_page[:last_m3u8+1]
                    pages.append(valid_page)
                    # å¤„ç†å‰©ä½™å†…å®¹
                    current_page = current_page[last_m3u8+1:] + [line]
                    current_length = sum(len(l)+1 for l in current_page)
                    # é‡ç½®æœ€åä½ç½®
                    last_m3u8 = -1
                    # æ£€æŸ¥ç°æœ‰å†…å®¹
                    for idx, l in enumerate(current_page):
                        if l.strip().endswith('.m3u8'):
                            last_m3u8 = idx
                else:
                    # å¼ºåˆ¶åˆ†é¡µï¼ˆä¸ç¬¦åˆè¦æ±‚ï¼‰
                    pages.append(current_page)
                    current_page = [line]
                    current_length = line_len
                    last_m3u8 = -1 if not line.strip().endswith('.m3u8') else 0
            else:
                current_page.append(line)
                current_length += line_len
                if line.strip().endswith('.m3u8'):
                    last_m3u8 = len(current_page) - 1

        # å¤„ç†æœ€åä¸€é¡µ
        if current_page:
            if last_m3u8 != -1:
                pages.append(current_page[:last_m3u8+1])
                # é€’å½’å¤„ç†å‰©ä½™è¡Œ
                pages += self._split_into_pages(current_page[last_m3u8+1:])
            else:
                pages.append(current_page)

        return pages

    async def _common_handler(self, event, api_urls, keyword):
        """åˆå¹¶å¤šAPIç»“æœå¹¶åˆ†é¡µçš„æ ¸å¿ƒé€»è¾‘"""
        total_attempts = len(api_urls)
        successful_apis = 0
        grouped_results = {}
        ordered_titles = []
        
        for api_url in api_urls:
            api_url = api_url.strip()
            if not api_url:
                continue

            encoded_keyword = urllib.parse.quote(keyword)
            query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(query_url, timeout=15) as response:
                        if response.status != 200:
                            continue

                        html_content = await response.text()
                        parsed_items = self._parse_html(html_content)
                        
                        if parsed_items:
                            successful_apis += 1
                            for title, url in parsed_items:
                                if title not in grouped_results:
                                    grouped_results[title] = []
                                    ordered_titles.append(title)
                                grouped_results[title].append(url)

            except Exception as e:
                self.context.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
                continue

        # æ„å»ºç»“æœ
        result_lines = []
        total_videos = sum(len(urls) for urls in grouped_results.values())
        
        for idx, title in enumerate(ordered_titles, 1):
            urls = grouped_results.get(title, [])
            result_lines.append(f"{idx}. ã€{title}ã€‘")
            result_lines.extend([f"   ğŸ¬ {url}" for url in urls])

        # åˆ†é¡µé€»è¾‘
        header = [
            f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
            f"ğŸ“Š æ‰¾åˆ° {total_videos} æ¡èµ„æº",
            "â”" * 30
        ]
        footer = [
            "â”" * 30,
            "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
            "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
            "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
            "â”" * 30
        ]

        if not result_lines:
            yield event.plain_result(f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº")
            return

        # åˆ†é¡µå¤„ç†
        pages = self._split_into_pages(result_lines)
        constructed_pages = []
        total_pages = len(pages)

        for idx, page in enumerate(pages, 1):
            content = []
            if idx == 1:
                content.extend(header)
            content.extend(page)
            if idx == total_pages:
                content.extend(footer)
            
            # æ·»åŠ åˆ†é¡µä¿¡æ¯
            page_info = f"ğŸ“„ ç¬¬ {idx}/{total_pages} é¡µ"
            if idx < total_pages:
                content.append(f"{page_info}\nå›å¤æ•°å­—ç»§ç»­æŸ¥çœ‹ï¼ˆ20ç§’å†…æœ‰æ•ˆï¼‰")
            else:
                content.append(page_info)
            
            constructed_page = "\n".join(content)
            constructed_pages.append(constructed_page)

        # å­˜å‚¨ç”¨æˆ·çŠ¶æ€
        user_id = str(event.user_id)
        USER_STATES[user_id] = {
            "pages": constructed_pages,
            "timestamp": time.time(),
            "total": total_pages
        }

        # å‘é€ç¬¬ä¸€é¡µ
        yield event.plain_result(constructed_pages[0])

    def _parse_html(self, html_content):
        """è§£æHTMLå¹¶è¿”å›ç»“æ„åŒ–æ•°æ®"""
        soup = BeautifulSoup(html_content, 'html.parser')
        video_items = soup.select('rss list video')[:self.records]
        
        parsed_data = []
        for item in video_items:
            title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
            for dd in item.select('dl > dd'):
                for url in dd.text.split('#'):
                    if url := url.strip():
                        parsed_data.append((title, url))
        return parsed_data

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šå½±è§†æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_vod, text):
            yield msg

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """ğŸ”å†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("ğŸ”æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_18, text):
            yield msg

    @filter.text
    async def handle_pagination(self, event: AstrMessageEvent, text: str):
        """å¤„ç†åˆ†é¡µè¯·æ±‚"""
        user_id = str(event.user_id)
        state = USER_STATES.get(user_id)

        if not state:
            return

        # æ£€æŸ¥è¶…æ—¶
        if time.time() - state["timestamp"] > 20:
            del USER_STATES[user_id]
            return

        # éªŒè¯è¾“å…¥
        if not text.isdigit():
            return
        
        page_num = int(text)
        if not 1 <= page_num <= state["total"]:
            yield event.plain_result(f"âš ï¸ é¡µç æ— æ•ˆï¼ˆ1-{state['total']}ï¼‰")
            del USER_STATES[user_id]
            return

        # å‘é€å¯¹åº”é¡µ
        yield event.plain_result(state["pages"][page_num-1])
        del USER_STATES[user_id]
