from astrbot.api.message_components import *
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import aiohttp
import urllib.parse
from bs4 import BeautifulSoup
import time
import asyncio
import re

@register("bot_vod", "appale", "è§†é¢‘æœç´¢åŠåˆ†é¡µåŠŸèƒ½ï¼ˆå‘½ä»¤ï¼š/vod /vodd /ç¿»é¡µï¼‰", "2.0.5")
class VideoSearchPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        self.api_url_vod = config.get("api_url_vod", "").split(',')
        self.api_url_18 = config.get("api_url_18", "").split(',')
        self.records = int(config.get("records", "3"))
        self.user_pages = {}
        self.lock = asyncio.Lock()

    def _get_user_identity(self, event: AstrMessageEvent) -> str:
        """å¢å¼ºç”¨æˆ·æ ‡è¯†è·å–"""
        try:
            if event.platform == "wechat":
                return f"wechat-{event.user.openid}"
            return f"{event.platform}-{event.get_sender_id()}"
        except Exception as e:
            self.context.logger.error(f"æ ‡è¯†è·å–å¼‚å¸¸: {str(e)}")
            return f"unknown-{int(time.time())}"

    async def _common_handler(self, event: AstrMessageEvent, api_urls: list, keyword: str):
        """æ ¸å¿ƒæœç´¢é€»è¾‘ï¼ˆæ™ºèƒ½åˆ†é¡µä¼˜åŒ–ç‰ˆï¼‰"""
        async with self.lock:
            user_id = self._get_user_identity(event)
            total_attempts = len(api_urls)
            successful_apis = 0
            grouped_results = {}
            ordered_titles = []

            # APIè¯·æ±‚å¤„ç†
            for api_url in api_urls:
                api_url = api_url.strip()
                if not api_url:
                    continue

                try:
                    encoded_keyword = urllib.parse.quote(keyword)
                    query_url = f"{api_url}?ac=videolist&wd={encoded_keyword}"
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.get(query_url, timeout=15) as response:
                            if response.status != 200:
                                continue
                                
                            html_content = await response.text()
                            soup = BeautifulSoup(html_content, 'html.parser')
                            video_items = soup.select('rss list video')[:self.records]
                            
                            for item in video_items:
                                title = item.select_one('name').text.strip() if item.select_one('name') else "æœªçŸ¥æ ‡é¢˜"
                                for dd in item.select('dl > dd'):
                                    for url in dd.text.split('#'):
                                        if (url := url.strip()):
                                            if title not in grouped_results:
                                                grouped_results[title] = []
                                                ordered_titles.append(title)
                                            grouped_results[title].append({
                                                "url": url,
                                                "is_m3u8": url.endswith('.m3u8')
                                            })
                            successful_apis += 1
                            
                except Exception as e:
                    self.context.logger.error(f"APIè¯·æ±‚é”™è¯¯: {str(e)}")
                    continue

            # æ„å»ºç»“æ„åŒ–ç»“æœ
            structured_results = []
            for idx, title in enumerate(ordered_titles, 1):
                entries = grouped_results.get(title, [])
                structured_results.append({
                    "title": f"{idx}. ã€{title}ã€‘",
                    "urls": entries
                })

            # æ™ºèƒ½åˆ†é¡µå¤„ç†ï¼ˆåŠ¨æ€åˆå¹¶ä¼˜åŒ–ï¼‰
            pages = []
            if structured_results:
                header = [
                    f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª",
                    f"ğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº",
                    "â”" * 28
                ]
                footer_base = [
                    "â”" * 28,
                    "ğŸ’¡ æ’­æ”¾æç¤ºï¼š",
                    "1. ç§»åŠ¨ç«¯ç›´æ¥ç²˜è´´é“¾æ¥åˆ°æµè§ˆå™¨",
                    "2. ç”µè„‘ç«¯æ¨èä½¿ç”¨PotPlayer/VLCæ’­æ”¾",
                    "3. ä½¿ç”¨:/ç¿»é¡µ é¡µç (è·³è½¬é¡µé¢)",
                    "â”" * 28
                ]

                # æœ‰æ•ˆæœŸè®¡ç®—
                expiry_timestamp = time.time() + 300
                beijing_time = time.strftime("%H:%M", time.gmtime(expiry_timestamp + 8 * 3600))
                
                # åŸºç¡€é•¿åº¦è®¡ç®—
                header_content = '\n'.join(header)
                header_length = len(header_content) + 1  # åŒ…å«æ¢è¡Œç¬¦
                base_footer = [
                    f"â° æœ‰æ•ˆæœŸè‡³ {beijing_time}ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰",
                    *footer_base
                ]
                footer_content = '\n'.join(base_footer)
                base_footer_length = len(footer_content) + 30  # é¢„ç•™é¡µç ç©ºé—´

                current_blocks = []
                remaining_blocks = structured_results.copy()
                min_page_size = 200  # æœ€å°é¡µé¢å†…å®¹é˜ˆå€¼

                while remaining_blocks:
                    # åˆå§‹åŒ–å½“å‰é¡µ
                    current_page = []
                    current_length = header_length
                    page_filled = False
                    
                    # åŠ¨æ€å¡«å……ç­–ç•¥
                    while remaining_blocks and not page_filled:
                        next_block = remaining_blocks[0]
                        
                        # ç”Ÿæˆå—å†…å®¹
                        block_lines = [next_block["title"]]
                        block_lines.extend([f"   ğŸ¬ {u['url']}" for u in next_block["urls"]])
                        block_content = '\n'.join(block_lines)
                        
                        # è®¡ç®—å—å°ºå¯¸
                        block_size = len(block_content) + (1 if current_page else 0)  # å—é—´æ¢è¡Œ
                        estimated_total = current_length + block_size + base_footer_length
                        
                        # å¡«å……æ¡ä»¶åˆ¤æ–­
                        if (current_length + block_size + base_footer_length <= 1200) or \
                           (not current_page and estimated_total <= 1500):
                            # æ·»åŠ å—åˆ°å½“å‰é¡µ
                            if current_page:
                                current_page.append('')  # å—é—´ç©ºè¡Œ
                            current_page.extend(block_lines)
                            current_length += block_size
                            remaining_blocks.pop(0)
                            
                            # æ£€æŸ¥åç»­å°å—æ˜¯å¦å¯ä»¥åˆå¹¶
                            lookahead_blocks = 3  # é¢„çœ‹åç»­3ä¸ªå—
                            for _ in range(min(lookahead_blocks, len(remaining_blocks))):
                                test_block = remaining_blocks[0]
                                test_lines = [test_block["title"]] + [f"   ğŸ¬ {u['url']}" for u in test_block["urls"]]
                                test_size = len('\n'.join(test_lines)) + 1  # æ¢è¡Œç¬¦
                                
                                if current_length + test_size + base_footer_length <= 1000:
                                    current_page.append('')
                                    current_page.extend(test_lines)
                                    current_length += test_size
                                    remaining_blocks.pop(0)
                                else:
                                    break
                        else:
                            page_filled = True

                    # ç”Ÿæˆé¡µé¢å†…å®¹
                    if current_page:
                        # æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦è¿‡å°
                        if len('\n'.join(current_page)) < min_page_size and remaining_blocks:
                            # å°è¯•åˆå¹¶ä¸‹ä¸€ä¸ªå—
                            next_block = remaining_blocks[0]
                            test_lines = [next_block["title"]] + [f"   ğŸ¬ {u['url']}" for u in next_block["urls"]]
                            test_size = len('\n'.join(test_lines)) + 1
                            
                            if current_length + test_size + base_footer_length <= 1500:
                                current_page.append('')
                                current_page.extend(test_lines)
                                current_length += test_size
                                remaining_blocks.pop(0)

                        # æ„å»ºé¡µè„š
                        page_number = len(pages) + 1
                        footer = [
                            f"ğŸ“‘ ç¬¬ {page_number}/PAGES é¡µ",
                            *base_footer
                        ]
                        full_content = '\n'.join([header_content] + current_page + footer)
                        pages.append(full_content)

                # æ›´æ–°æ€»é¡µæ•°
                total_pages = len(pages)
                for i in range(len(pages)):
                    pages[i] = pages[i].replace("PAGES", str(total_pages))
                    
                # å­˜å‚¨åˆ†é¡µæ•°æ®
                self.user_pages[user_id] = {
                    "pages": pages,
                    "timestamp": time.time(),
                    "total_pages": total_pages,
                    "search_info": f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\nğŸ“Š æ‰¾åˆ° {sum(len(g['urls']) for g in structured_results)} æ¡èµ„æº"
                }
                yield event.plain_result(pages[0])
            else:
                yield event.plain_result(f"ğŸ” æœç´¢ {total_attempts} ä¸ªæºï½œæˆåŠŸ {successful_apis} ä¸ª\n{'â”'*30}\næœªæ‰¾åˆ°ç›¸å…³èµ„æº")

    @filter.command("vod")
    async def search_normal(self, event: AstrMessageEvent, text: str):
        """æ™®é€šè§†é¢‘æœç´¢"""
        if not self.api_url_vod:
            yield event.plain_result("âš ï¸ æ™®é€šè§†é¢‘æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_vod, text):
            yield msg

    @filter.command("vodd")
    async def search_adult(self, event: AstrMessageEvent, text: str):
        """æˆäººå†…å®¹æœç´¢"""
        if not self.api_url_18:
            yield event.plain_result("ğŸ” æˆäººå†…å®¹æœåŠ¡æœªå¯ç”¨")
            return
        async for msg in self._common_handler(event, self.api_url_18, text):
            yield msg

    @filter.command("ç¿»é¡µ")
    async def paginate_results(self, event: AstrMessageEvent, text: str):
        """æ™ºèƒ½åˆ†é¡µæ§åˆ¶"""
        user_id = self._get_user_identity(event)
        async with self.lock:
            page_data = self.user_pages.get(user_id)

            if not page_data or (time.time() - page_data["timestamp"]) > 300:
                yield event.plain_result("â³ æœç´¢ç»“æœå·²è¿‡æœŸï¼ˆæœ‰æ•ˆæœŸ5åˆ†é’Ÿï¼‰ï¼Œè¯·é‡æ–°æœç´¢")
                return

            try:
                page_num = int(text.strip())
                if not 1 <= page_num <= page_data["total_pages"]:
                    raise ValueError
            except ValueError:
                yield event.plain_result(f"âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé¡µç ï¼ˆ1-{page_data['total_pages']}ï¼‰")
                return

            # åŠ¨æ€æ›´æ–°æœ‰æ•ˆæœŸ
            new_expiry = time.time() + 300
            new_beijing_time = time.strftime("%H:%M", time.gmtime(new_expiry + 8 * 3600))
            
            # ä½¿ç”¨æ­£åˆ™ç²¾ç¡®æ›¿æ¢æ—¶é—´
            old_time_pattern = r"â° æœ‰æ•ˆæœŸè‡³ \d{2}:\d{2}"
            updated_content = re.sub(
                old_time_pattern, 
                f"â° æœ‰æ•ˆæœŸè‡³ {new_beijing_time}", 
                page_data["pages"][page_num-1]
            )
            
            # ä¿æŒæ€»æœ‰æ•ˆæœŸä¸å˜
            self.user_pages[user_id]["timestamp"] = new_expiry - 300
            yield event.plain_result(updated_content)

    async def _clean_expired_records(self):
        """è‡ªåŠ¨æ¸…ç†ä»»åŠ¡"""
        while True:
            async with self.lock:
                now = time.time()
                expired_users = [
                    uid for uid, data in self.user_pages.items()
                    if now - data["timestamp"] > 300
                ]
                for uid in expired_users:
                    del self.user_pages[uid]
            await asyncio.sleep(60)

    async def activate(self):
        """æ’ä»¶æ¿€æ´»"""
        await super().activate()
        asyncio.create_task(self._clean_expired_records())
